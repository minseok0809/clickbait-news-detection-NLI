{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install BeautifulSoup4\n",
    "%pip install ipywidgets\n",
    "%jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import make_train_valid_json_xlsx_file_path_list\n",
    "from data_preprocessing import write_jsontext_to_xlsx_file_with_batch_size\n",
    "from data_preprocessing import write_spreadsheettext_to_list_merge_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['data/aihub_clickbait_detection/Training/Labeling'+ '/**/*.json', \n",
    "                  'data/aihub_clickbait_detection/Validation/Labeling'+ '/**/*.json']\n",
    "xlsx_path_list = ['data/aihub_clickbait_detection/df13/clickbait_classfication_train_part1_', \n",
    "                 'data/aihub_clickbait_detection/df13/clickbait_classfication_valid_part1_',\n",
    "                 'data/aihub_clickbait_detection/df13/clickbait_classfication_train_part2_', \n",
    "                 'data/aihub_clickbait_detection/df13/clickbait_classfication_valid_part2_',\n",
    "                 'data/aihub_clickbait_detection/df13/nonclickbait_classfication_train_part1_', \n",
    "                 'data/aihub_clickbait_detection/df13/nonclickbait_classfication_valid_part1_',\n",
    "                'data/aihub_clickbait_detection/df13/nonclickbait_classfication_train_part2_', \n",
    "                 'data/aihub_clickbait_detection/df13/nonclickbait_classfication_valid_part2_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train clickbait json file: 296035\n",
      "The number of valid clickbait json file: 36991\n",
      "The number of clickbait json file: 333026\n",
      "\n",
      "The number of train nonclickbait json file: 290689\n",
      "The number of valid nonclickbait json file: 36330\n",
      "The number of nonclickbait json file: 327019\n",
      "\n",
      "The number of train json file: 586724\n",
      "The number of valid json file: 73321\n",
      "The number of json file: 660045\n"
     ]
    }
   ],
   "source": [
    "train_part1_clickbait_json_file_path_list, train_part1_nonclickbait_json_file_path_list, \\\n",
    "train_part2_clickbait_json_file_path_list, train_part2_nonclickbait_json_file_path_list, \\\n",
    "valid_part1_clickbait_json_file_path_list, valid_part1_nonclickbait_json_file_path_list, \\\n",
    "valid_part2_clickbait_json_file_path_list, valid_part2_nonclickbait_json_file_path_list, \\\n",
    "train_part1_clickbait_xlsx_file_path_list, train_part1_nonclickbait_xlsx_file_path_list, \\\n",
    "train_part2_clickbait_xlsx_file_path_list, train_part2_nonclickbait_xlsx_file_path_list, \\\n",
    "valid_part1_clickbait_xlsx_file_path_list, valid_part1_nonclickbait_xlsx_file_path_list, \\\n",
    "valid_part2_clickbait_xlsx_file_path_list, valid_part2_nonclickbait_xlsx_file_path_list \\\n",
    "= make_train_valid_json_xlsx_file_path_list(json_path_list, xlsx_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Size]\n",
      "The number of xlsx file: 146\n",
      "\n",
      "[Order]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [34:17<00:00, 13.99s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(train_part1_clickbait_json_file_path_list, train_part1_clickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(train_part1_nonclickbait_json_file_path_list, train_part1_nonclickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(train_part2_clickbait_json_file_path_list, train_part2_clickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(train_part2_nonclickbait_json_file_path_list, train_part2_nonclickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(valid_part1_clickbait_json_file_path_list, valid_part1_clickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(valid_part1_nonclickbait_json_file_path_list, valid_part1_nonclickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(valid_part2_clickbait_json_file_path_list, valid_part2_clickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(valid_part2_nonclickbait_json_file_path_list, valid_part2_nonclickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [00:26<00:00, 10.91it/s]\n",
      "100%|██████████| 38/38 [00:03<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train text: 291458\n",
      "The number of valid text: 18210\n",
      "The number of test text: 18212\n",
      "Train Data: Label\n",
      "D    48895\n",
      "B    48867\n",
      "C    48521\n",
      "A    48495\n",
      "E    48381\n",
      "F    48299\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valid Data: Label\n",
      "F    3214\n",
      "D    3037\n",
      "C    3018\n",
      "A    2991\n",
      "E    2981\n",
      "B    2969\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Data: Label\n",
      "E    3175\n",
      "A    3115\n",
      "C    3040\n",
      "D    3034\n",
      "B    3008\n",
      "F    2840\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [00:40<00:00,  7.23it/s]\n",
      "100%|██████████| 38/38 [00:05<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train text: 291458\n",
      "The number of valid text: 18210\n",
      "The number of test text: 18212\n",
      "Train Data: Label\n",
      "C    41939\n",
      "B    41865\n",
      "E    41705\n",
      "F    41612\n",
      "A    41533\n",
      "D    41467\n",
      "G    41337\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valid Data: Label\n",
      "F    2785\n",
      "D    2631\n",
      "A    2592\n",
      "G    2579\n",
      "E    2570\n",
      "B    2559\n",
      "C    2494\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Data: Label\n",
      "D    2755\n",
      "B    2639\n",
      "E    2639\n",
      "A    2608\n",
      "G    2599\n",
      "C    2566\n",
      "F    2406\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spreadsheet_folder = './data/aihub_clickbait_detection/df13/'\n",
    "dataset_folder = './data/aihub_clickbait_detection/dataset13/'\n",
    "\n",
    "# write_spreadsheettext_to_list_merge_file(spreadsheet_folder, dataset_folder, '_three_option.csv'); print()\n",
    "# write_spreadsheettext_to_list_merge_file(spreadsheet_folder, dataset_folder, '_four_option.csv'); print()\n",
    "# write_spreadsheettext_to_list_merge_file(spreadsheet_folder, dataset_folder, '_five_option.csv'); print()\n",
    "write_spreadsheettext_to_list_merge_file(spreadsheet_folder, dataset_folder, '_six_option.csv'); print()\n",
    "write_spreadsheettext_to_list_merge_file(spreadsheet_folder, dataset_folder, '_seven_option.csv'); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data</b>\n",
    "<br>[AI Hub 낚시성 기사 탐지 데이터](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=71338)\n",
    "\n",
    "<br><b>Paper</b>\n",
    "<br>[Yinhan Liu et al. Robustly Optimized BERT Pretraining Approach. 2019.](https://aclanthology.org/N16-1174/)\n",
    "<br>[Kevin Clark et al. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. ICLR, 2020](https://arxiv.org/abs/2003.10555)\n",
    "<br>[Zichao Yang et al. Hierarchical Attention Networks for Document Classification. NAACL, 2016.](https://aclanthology.org/N16-1174/)\n",
    "<br>[Yongjie Wang et al. On the Use of Bert for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation. NAACL, 2022.](https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11485497)\n",
    "<br>[한상우, 온병원. 다중 계층 BERT를 활용한 낚시성 기사 탐지 모델. 한국정보기술학회, 2023.](https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11485497)\n",
    "\n",
    "<br><b>HuggingFace</b>\n",
    "<br>[klue/roberta-base](https://huggingface.co/klue/roberta-base)\n",
    "<br>[klue/roberta-large](https://huggingface.co/klue/roberta-large)\n",
    "\n",
    "<br><b>Github</b>\n",
    "<br>[hhk1364/Fishing-Article-Detection-Project-Backend](https://github.com/hhk1364/Fishing-Article-Detection-Project-Backend)\n",
    "<br>[SKTBrain/KoBERT](https://github.com/SKTBrain/KoBERT)\n",
    "<br>[monologg/KoBERT-Transformers](https://github.com/monologg/KoBERT-Transformers)\n",
    "<br>[monologg/KoELECTRA](https://github.com/monologg/KoELECTRA)\n",
    "<br>[jaehyeongAN/KoELECTRA-finetuned-sentiment-analysis](https://github.com/jaehyeongAN/KoELECTRA-finetuned-sentiment-analysis)\n",
    "<br>[lkkaram/korean-frown-sentence-classifier](https://github.com/lkkaram/korean-frown-sentence-classifier)\n",
    "<br>[kimwoonggon/publicservant_AI](https://github.com/kimwoonggon/publicservant_AI/)\n",
    "<br>[lingochamp/Multi-Scale-BERT-AES](https://github.com/lingochamp/Multi-Scale-BERT-AES/)\n",
    "<br>[Doheon/NewsClassification-KoBERT](https://github.com/Doheon/NewsClassification-KoBERT)\n",
    "<br>[jaylnne/nsmc-bert-pytorch_lightning](https://github.com/jaylnne/nsmc-bert-pytorch_lightning)\n",
    "\n",
    "<br><b>Blog</b>\n",
    "<br>[Week 28 - BERT만 잘 써먹어도 최고가 될 수 있다?](https://jiho-ml.com/weekly-nlp-28/)\n",
    "<br>[Week 37 - NLP 모델, 낚시성 기사 방지 효과 검증돼](https://jiho-ml.com/weekly-nlp-37/)\n",
    "\n",
    "<br><b>Dacon</b>\n",
    "<br>[[private 3rd] klue/roberta + teacher-student](https://dacon.io/en/competitions/official/235747/codeshare/3072)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
